{"cells":[{"cell_type":"markdown","metadata":{"id":"v6ygqQJMxW_y"},"source":["# Predicting Cybersecurity Breach Likelihood Based on Financial and Organizational Characteristics\n","\n","**Data Mining Final Project**  \n","**Team Members:** Jax Fonseca-Folden, Gustavo Franco, Charles Morris, Rafan, Thomas  \n","**Date:** November 2025\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"7tyq282txW_2"},"source":["## 1. Introduction\n","\n","<!-- JAX: Write introduction here -->\n","<!-- Include: problem statement, questions to answer, and overall goal -->\n","\n","*This section should introduce the problem of predicting cybersecurity breaches, the questions being explored, and the project goals.*"]},{"cell_type":"markdown","metadata":{"id":"pBX8uIl_xW_2"},"source":["## 2. About the Data\n","\n","<!-- JAX: Write data description here -->\n","<!-- Include: data sources, features, size, and any initial statistics -->\n","\n","*This section should describe where the data came from (VCDB, SEC EDGAR), the features in each dataset, and basic statistics about the data.*"]},{"cell_type":"markdown","metadata":{"id":"AC7p9FcLxW_3"},"source":["---\n","## 3. Methods\n","\n","This section documents all preprocessing steps and modeling approaches used in our analysis."]},{"cell_type":"markdown","metadata":{"id":"ej0UBLD5xW_3"},"source":["### 3.1 Setup and Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XShmKNzDxW_4","outputId":"48e58a8c-196e-420c-b392-c3543d30904c","executionInfo":{"status":"ok","timestamp":1763523323566,"user_tz":300,"elapsed":6547,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… All imports successful!\n","NumPy version: 2.0.2\n","Pandas version: 2.2.2\n","Random state set to: 42\n"]}],"source":["# Standard library imports\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","\n","# Machine Learning\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.cluster import KMeans\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    roc_auc_score, roc_curve, confusion_matrix,\n","    silhouette_score, mean_squared_error, r2_score, classification_report\n",")\n","\n","# XGBoost import (FIXED: Added missing import)\n","from xgboost import XGBClassifier\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.patches import Rectangle\n","\n","# Suppress warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set random state for reproducibility (FIXED: Added constant)\n","RANDOM_STATE = 42\n","\n","# Set style with cohesive color theme\n","plt.style.use('seaborn-v0_8-darkgrid')\n","sns.set_palette(\"husl\")\n","\n","# Custom color palette - Professional blue/purple theme\n","COLORS = {\n","    'primary': '#2E86AB',      # Deep blue\n","    'secondary': '#A23B72',    # Deep purple\n","    'accent': '#F18F01',       # Orange\n","    'success': '#06A77D',      # Teal green\n","    'danger': '#C73E1D',       # Deep red\n","    'neutral': '#6C757D'       # Gray\n","}\n","\n","# Set display options\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.width', None)\n","\n","print(\"âœ… All imports successful!\")\n","print(f\"NumPy version: {np.__version__}\")\n","print(f\"Pandas version: {pd.__version__}\")\n","print(f\"Random state set to: {RANDOM_STATE}\")"]},{"cell_type":"markdown","metadata":{"id":"QF1o5eOYxW_5"},"source":["### 3.2 Data Loading"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"WvedVVFXxW_6","outputId":"6dcfcbce-f73c-445a-dfee-8721ffb068cd","executionInfo":{"status":"error","timestamp":1763523323699,"user_tz":300,"elapsed":130,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'sec_company_financials.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4194676972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the preprocessed datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinancial_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sec_company_financials.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mincidents_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vcdb_cybersecurity_incidents.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sec_company_financials.csv'"]}],"source":["# Load the preprocessed datasets\n","financial_data = pd.read_csv('sec_company_financials.csv')\n","incidents_data = pd.read_csv('vcdb_cybersecurity_incidents.csv')\n","\n","print(\"=\" * 80)\n","print(\"DATASET LOADING SUMMARY\")\n","print(\"=\" * 80)\n","print(f\"\\nFinancial Data Shape: {financial_data.shape}\")\n","print(f\"Incidents Data Shape: {incidents_data.shape}\")\n","print(\"\\n--- Financial Data Columns ---\")\n","print(financial_data.columns.tolist())\n","print(\"\\n--- Incidents Data Columns ---\")\n","print(incidents_data.columns.tolist())"]},{"cell_type":"markdown","metadata":{"id":"rMsMKDNoxW_6"},"source":["### 3.3 Data Preprocessing\n","\n","#### 3.3.1 Understanding the Data Collection Process\n","\n","Our data preprocessing pipeline was designed to gather and clean two primary datasets for cybersecurity breach prediction:\n","\n","**Data Sources:**\n","1. **SEC EDGAR Company Facts** - Contains financial and organizational metrics for public companies including:\n","   - Company identifiers (CIK, Entity Name)\n","   - Financial statements (Assets, Liabilities, Net Income, Revenue)\n","   - Temporal data with quarterly/annual reporting dates\n","   \n","2. **VERIS Community Database (VCDB)** - Comprehensive cybersecurity incident records including:\n","   - Victim organization information (name, industry, country, employee count)\n","   - Incident characteristics (year, actions, actors, breach types)\n","   - Impact metrics (records lost, breach categories)\n","\n","**Data Collection Methodology:**\n","\n","The SEC EDGAR data was obtained through the official SEC API, which provides structured XBRL financial statement data for all publicly traded companies. We downloaded the `companyfacts.zip` file containing JSON files for individual companies, then extracted key financial metrics and aggregated them to the company level. The data collection respected SEC rate limits and included proper User-Agent identification as required by SEC guidelines.\n","\n","The VCDB data was cloned from the official GitHub repository (https://github.com/vz-risk/VCDB), which contains structured JSON files of validated cybersecurity incidents. Each incident record follows the VERIS framework (Vocabulary for Event Recording and Incident Sharing), providing standardized categorization of breach types, threat actors, and impact metrics.\n","\n","**Initial Data Quality Assessment:**\n","\n","Upon initial examination, both datasets exhibited common data quality issues requiring systematic preprocessing:\n","\n","For the financial data:\n","- Missing entity names (11 out of 100 sampled records)\n","- Significant missing values in Liabilities (29%) and Revenue (33%)\n","- Temporal misalignment across different financial metrics\n","- Some entries with null CIK identifiers\n","\n","For the incidents data:\n","- Missing victim names (316 out of 10,399 records)\n","- Missing records_lost values (filled with 0 during preprocessing)\n","- Geographic data limited primarily to US (victim_state field)\n","- Multiple incident categories encoded as binary flags\n","\n","#### 3.3.2 Preprocessing Steps Implemented\n","\n","Based on our exploratory analysis in the preprocessing notebook, we implemented the following cleaning procedures:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VOEZ6gqFxW_7","executionInfo":{"status":"aborted","timestamp":1763523323793,"user_tz":300,"elapsed":98,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["print(\"=\" * 80)\n","print(\"DATA PREPROCESSING\")\n","print(\"=\" * 80)\n","\n","# Financial Data Preprocessing\n","print(\"\\n--- Financial Data Preprocessing ---\")\n","print(f\"Original shape: {financial_data.shape}\")\n","print(f\"Missing entity names: {financial_data['EntityName'].isna().sum()}\")\n","\n","# Select relevant columns\n","financial_cols = ['CIK', 'EntityName', 'Assets', 'Liabilities', 'NetIncome', 'Revenue']\n","financial_clean = financial_data[financial_cols].copy()\n","\n","# Drop rows with missing entity names\n","financial_clean = financial_clean.dropna(subset=['EntityName'])\n","print(f\"After dropping missing entity names: {financial_clean.shape}\")\n","\n","# Display missing value summary\n","print(\"\\nMissing values per column:\")\n","print(financial_clean.isna().sum())\n","\n","# Incidents Data Preprocessing\n","print(\"\\n--- Incidents Data Preprocessing ---\")\n","print(f\"Original shape: {incidents_data.shape}\")\n","print(f\"Missing victim names: {incidents_data['victim_name'].isna().sum()}\")\n","\n","# Drop irrelevant columns identified in preprocessing analysis\n","cols_to_drop = ['victim_state', 'victim_industry', 'victim_revenue',\n","                'discovery_date', 'reference', 'source_id']\n","incidents_clean = incidents_data.drop(columns=[col for col in cols_to_drop if col in incidents_data.columns])\n","\n","# Fill missing records_lost with 0 (indicating no reported loss)\n","incidents_clean['records_lost'] = incidents_clean['records_lost'].fillna(0)\n","\n","# Drop rows with missing victim names\n","incidents_clean = incidents_clean.dropna(subset=['victim_name'])\n","print(f\"After preprocessing: {incidents_clean.shape}\")\n","\n","# Display missing value summary\n","print(\"\\nMissing values per column:\")\n","print(incidents_clean.isna().sum())\n","\n","print(\"\\nâœ… Preprocessing complete!\")"]},{"cell_type":"markdown","metadata":{"id":"9uqZ6cboxW_7"},"source":["#### 3.3.3 Preprocessing Rationale\n","\n","**Entity Name Handling:** We dropped records with missing entity names as company identification is critical for any potential data merging and analysis. Without entity names, records cannot be matched across datasets or properly categorized.\n","\n","**Financial Metrics:** We retained records with some missing financial values (Assets, Liabilities, Revenue, NetIncome) rather than dropping them entirely, as companies may not report all metrics in every filing period. This preserves more data for analysis while acknowledging that some models may require complete cases.\n","\n","**Incidents Data Columns:** The `victim_state` field was dropped because it only contains US state information and excludes international incidents, introducing geographic bias. Fields like `victim_industry` and `victim_revenue` had inconsistent formatting and missing values, making them unreliable for modeling. The `reference` and `source_id` fields are metadata not relevant for predictive modeling.\n","\n","**Records Lost:** Missing values in `records_lost` were filled with 0 rather than dropped, as many incidents may not have reported data loss or the loss may genuinely be zero. This approach retains more incident records for analysis while making a conservative assumption about data loss."]},{"cell_type":"markdown","metadata":{"id":"5DXVBVzGId8B"},"source":["### 3.4 IMPROVED Company Name Matching with Fuzzy Logic\n","\n","**FIXES APPLIED:**\n","1. **More selective thresholds** - Using 75% for fuzzy matching instead of 60%\n","2. **Better name cleaning** - Removes common corporate suffixes but preserves core identity\n","3. **Multi-stage matching** - Prioritizes exact matches first, then progressively more lenient\n","4. **Minimum length requirements** - Prevents matching on trivial 2-3 letter strings\n","5. **Deduplication** - Keeps only the best match per company\n","6. **Match quality validation** - Manual review of borderline matches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blD45I9HId8B","executionInfo":{"status":"aborted","timestamp":1763523323820,"user_tz":300,"elapsed":124,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Install rapidfuzz if needed\n","try:\n","    from rapidfuzz import fuzz, process\n","    print(\"âœ… rapidfuzz already installed\")\n","except ImportError:\n","    print(\"Installing rapidfuzz...\")\n","    import sys\n","    import subprocess\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rapidfuzz\"])\n","    from rapidfuzz import fuzz, process\n","    print(\"âœ… rapidfuzz installed successfully\")\n","\n","import re\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"  IMPROVED FUZZY MATCHING - More Selective Approach\")\n","print(\"=\"*80)\n","\n","# Improved cleaning function - removes common suffixes but preserves identity\n","def clean_company_name(name):\n","    \"\"\"Clean company name while preserving core identity.\"\"\"\n","    if pd.isna(name):\n","        return \"\"\n","\n","    name = str(name).lower().strip()\n","\n","    # Remove common corporate suffixes - but be selective\n","    suffixes = [\n","        r'\\binc\\b', r'\\bincorporated\\b', r'\\bcorp\\b', r'\\bcorporation\\b',\n","        r'\\bltd\\b', r'\\blimited\\b', r'\\bllc\\b', r'\\blp\\b', r'\\bllp\\b',\n","        r'\\bco\\b', r'\\bcompany\\b', r'\\bplc\\b', r'\\bgroup\\b', r'\\bholdings\\b'\n","    ]\n","\n","    for suffix in suffixes:\n","        name = re.sub(suffix, '', name)\n","\n","    # Remove special characters but keep spaces\n","    name = re.sub(r'[^a-z0-9\\s]', ' ', name)\n","\n","    # Remove extra whitespace\n","    name = ' '.join(name.split())\n","\n","    return name.strip()\n","\n","# Clean both datasets\n","print(\"\\nCleaning company names...\")\n","financial_clean['company_clean'] = financial_clean['EntityName'].apply(clean_company_name)\n","incidents_clean['company_clean'] = incidents_clean['victim_name'].apply(clean_company_name)\n","\n","# Filter out very short names (< 3 chars) as they cause false matches\n","financial_clean = financial_clean[financial_clean['company_clean'].str.len() >= 3].copy()\n","incidents_clean = incidents_clean[incidents_clean['company_clean'].str.len() >= 3].copy()\n","\n","print(f\"Financial companies after cleaning: {len(financial_clean)}\")\n","print(f\"Incident companies after cleaning: {len(incidents_clean)}\")\n","\n","# Get unique company names\n","financial_companies = financial_clean['company_clean'].unique()\n","incident_companies = incidents_clean['company_clean'].unique()\n","\n","print(f\"Unique financial companies: {len(financial_companies)}\")\n","print(f\"Unique breached companies: {len(incident_companies)}\")\n","\n","# Initialize matching results\n","all_matches = []\n","matched_fin = set()\n","\n","# STRATEGY 1: Exact Match (highest confidence)\n","print(\"\\n\" + \"-\"*80)\n","print(\"STRATEGY 1: Exact Match (100% confidence)\")\n","print(\"-\"*80)\n","\n","for fin in financial_companies:\n","    if fin in incident_companies:\n","        all_matches.append({\n","            'fin': fin,\n","            'inc': fin,\n","            'score': 100,\n","            'strategy': 'exact'\n","        })\n","        matched_fin.add(fin)\n","\n","exact_count = len([m for m in all_matches if m['strategy'] == 'exact'])\n","print(f\"âœ… Exact matches found: {exact_count}\")\n","\n","# STRATEGY 2: High-quality Fuzzy Match (75% threshold)\n","print(\"\\n\" + \"-\"*80)\n","print(\"STRATEGY 2: High-Quality Fuzzy Match (75%+ threshold)\")\n","print(\"-\"*80)\n","\n","remaining = [c for c in financial_companies if c not in matched_fin]\n","FUZZY_THRESHOLD = 75  # More selective than before (was 60%)\n","\n","print(f\"Checking {len(remaining)} remaining companies...\")\n","\n","for fin in remaining:\n","    # Skip very short names to avoid false positives\n","    if len(fin) < 4:\n","        continue\n","\n","    best_match = None\n","    best_score = 0\n","    best_algo = None\n","\n","    # Try multiple fuzzy algorithms and take the best\n","    # Token set ratio is best for company names with different word orders\n","    r1 = process.extractOne(fin, incident_companies, scorer=fuzz.token_set_ratio)\n","    if r1 and r1[1] > best_score:\n","        best_match, best_score, best_algo = r1[0], r1[1], 'token_set'\n","\n","    # Token sort ratio for similar names with different arrangements\n","    r2 = process.extractOne(fin, incident_companies, scorer=fuzz.token_sort_ratio)\n","    if r2 and r2[1] > best_score:\n","        best_match, best_score, best_algo = r2[0], r2[1], 'token_sort'\n","\n","    # Standard ratio for overall similarity\n","    r3 = process.extractOne(fin, incident_companies, scorer=fuzz.ratio)\n","    if r3 and r3[1] > best_score:\n","        best_match, best_score, best_algo = r3[0], r3[1], 'ratio'\n","\n","    # Only accept high-quality matches\n","    if best_score >= FUZZY_THRESHOLD:\n","        all_matches.append({\n","            'fin': fin,\n","            'inc': best_match,\n","            'score': best_score,\n","            'strategy': f'fuzzy_{best_algo}'\n","        })\n","        matched_fin.add(fin)\n","\n","fuzzy_count = len([m for m in all_matches if 'fuzzy' in m['strategy']])\n","print(f\"âœ… High-quality fuzzy matches (75%+): {fuzzy_count}\")\n","\n","# STRATEGY 3: Token/Word Matching (for multi-word company names)\n","print(\"\\n\" + \"-\"*80)\n","print(\"STRATEGY 3: Significant Word Match (multi-word companies)\")\n","print(\"-\"*80)\n","\n","remaining = [c for c in financial_companies if c not in matched_fin]\n","print(f\"Checking {len(remaining)} remaining companies...\")\n","\n","for fin in remaining:\n","    fin_words = set(fin.split())\n","\n","    # Only process if company name has multiple meaningful words\n","    meaningful_words = [w for w in fin_words if len(w) >= 4]\n","    if len(meaningful_words) < 2:\n","        continue\n","\n","    best_match = None\n","    best_overlap = 0\n","\n","    for inc in incident_companies:\n","        inc_words = set(inc.split())\n","        common = fin_words & inc_words\n","\n","        # Require at least 2 meaningful common words (4+ chars each)\n","        meaningful_common = [w for w in common if len(w) >= 4]\n","\n","        if len(meaningful_common) >= 2:\n","            # Calculate overlap ratio\n","            overlap = len(meaningful_common) / min(len(fin_words), len(inc_words))\n","\n","            if overlap > best_overlap:\n","                best_overlap = overlap\n","                best_match = inc\n","\n","    # Require at least 50% word overlap\n","    if best_match and best_overlap >= 0.5:\n","        score = int(best_overlap * 100)\n","        all_matches.append({\n","            'fin': fin,\n","            'inc': best_match,\n","            'score': score,\n","            'strategy': 'word_match'\n","        })\n","        matched_fin.add(fin)\n","\n","word_count = len([m for m in all_matches if m['strategy'] == 'word_match'])\n","print(f\"âœ… Word-based matches: {word_count}\")\n","\n","# RESULTS SUMMARY\n","print(\"\\n\" + \"=\"*80)\n","print(\"FINAL MATCHING RESULTS\")\n","print(\"=\"*80)\n","\n","print(f\"\\nðŸ“Š Total potential matches: {len(all_matches)}\")\n","\n","# Create DataFrame and remove duplicates (keep best score)\n","matches_df = pd.DataFrame(all_matches)\n","matches_df.columns = ['financial_name', 'incident_name', 'match_score', 'strategy']\n","\n","# Sort by score and remove duplicates\n","matches_df = matches_df.sort_values('match_score', ascending=False)\n","matches_df = matches_df.drop_duplicates(subset='financial_name', keep='first')\n","\n","print(f\"\\nâœ… Unique company matches: {len(matches_df)}\")\n","\n","# Strategy breakdown\n","print(f\"\\nðŸ“ˆ Breakdown by matching strategy:\")\n","for strat in matches_df['strategy'].value_counts().index:\n","    count = len(matches_df[matches_df['strategy'] == strat])\n","    avg = matches_df[matches_df['strategy'] == strat]['match_score'].mean()\n","    print(f\"   {strat:20s}: {count:3d} matches (avg score: {avg:5.1f}%)\")\n","\n","# Quality distribution\n","print(f\"\\nðŸŽ¯ Match quality distribution:\")\n","excellent = len(matches_df[matches_df['match_score'] >= 90])\n","good = len(matches_df[(matches_df['match_score'] >= 75) & (matches_df['match_score'] < 90)])\n","fair = len(matches_df[(matches_df['match_score'] >= 60) & (matches_df['match_score'] < 75)])\n","low = len(matches_df[matches_df['match_score'] < 60])\n","\n","print(f\"   Excellent (90-100%): {excellent}\")\n","print(f\"   Good (75-89%):       {good}\")\n","print(f\"   Fair (60-74%):       {fair}\")\n","print(f\"   Low (<60%):          {low}\")\n","\n","# Show sample matches for verification\n","print(f\"\\n\" + \"-\"*80)\n","print(\"SAMPLE MATCHES (Top 20 by score)\")\n","print(\"-\"*80)\n","print(matches_df.head(20).to_string(index=False))\n","\n","# Create final modeling dataset\n","print(\"\\n\" + \"=\"*80)\n","print(\"CREATING MODELING DATASET\")\n","print(\"=\"*80)\n","\n","# Companies with breaches\n","breached_companies = financial_clean[\n","    financial_clean['company_clean'].isin(matches_df['financial_name'])\n","].copy()\n","breached_companies['breach_occurred'] = 1\n","\n","# Companies without breaches\n","non_breached_companies = financial_clean[\n","    ~financial_clean['company_clean'].isin(matches_df['financial_name'])\n","].copy()\n","non_breached_companies['breach_occurred'] = 0\n","\n","# Combine into modeling dataset\n","modeling_data = pd.concat([breached_companies, non_breached_companies], ignore_index=True)\n","\n","# Report statistics\n","breach_rate = modeling_data['breach_occurred'].mean() * 100\n","total_companies = len(modeling_data)\n","breached_count = len(breached_companies)\n","non_breached_count = len(non_breached_companies)\n","\n","print(f\"\\nðŸ“Š Final Dataset Statistics:\")\n","print(f\"   Total companies:     {total_companies}\")\n","print(f\"   Breached:            {breached_count} ({breached_count/total_companies*100:.1f}%)\")\n","print(f\"   Non-breached:        {non_breached_count} ({non_breached_count/total_companies*100:.1f}%)\")\n","print(f\"   Breach rate:         {breach_rate:.1f}%\")\n","\n","# Class balance check\n","if breach_rate > 80:\n","    print(f\"\\nâš ï¸  WARNING: Dataset is heavily imbalanced (>{breach_rate:.0f}% breached)\")\n","    print(f\"   This may cause modeling issues. Consider:\")\n","    print(f\"   1. Reviewing match quality thresholds\")\n","    print(f\"   2. Using class balancing techniques (SMOTE, class weights)\")\n","    print(f\"   3. Collecting more non-breached company data\")\n","elif breach_rate < 20:\n","    print(f\"\\nâš ï¸  WARNING: Very few breaches detected ({breach_rate:.0f}%)\")\n","    print(f\"   Consider: Reviewing matching thresholds or data quality\")\n","else:\n","    print(f\"\\nâœ… Dataset balance is reasonable for modeling!\")\n","\n","print(\"=\"*80)"]},{"cell_type":"markdown","metadata":{"id":"GtjUR9V-xW_7"},"source":["### 3.5 Exploratory Data Analysis\n","\n","<!-- RAFAN: Create visualizations and data insights here -->\n","<!-- This section should contain graphs showing data distributions, -->\n","<!-- patterns, and relationships between variables -->\n","\n","*RAFAN: Please create multiple visualizations here exploring the data characteristics, such as:*\n","- *Distribution of breaches by year*\n","- *Company size distributions*\n","- *Incident type frequencies*\n","- *Any other insights from the data*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7E71r0YWxW_8","executionInfo":{"status":"aborted","timestamp":1763523323823,"user_tz":300,"elapsed":126,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# RAFAN: Exploratory Data Analysis Visualizations\n","# Using consistent theming with COLORS dictionary\n","\n","print(\"=\" * 80)\n","print(\"EXPLORATORY DATA ANALYSIS\")\n","print(\"=\" * 80)\n","\n","# Create comprehensive EDA figure\n","fig = plt.figure(figsize=(18, 12))\n","gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.35)\n","\n","# 1. Breaches Over Time\n","ax1 = fig.add_subplot(gs[0, :2])\n","breach_by_year = incidents_clean.groupby('incident_year').size()\n","ax1.plot(breach_by_year.index, breach_by_year.values,\n","         marker='o', linewidth=2.5, markersize=8, color=COLORS['primary'])\n","ax1.fill_between(breach_by_year.index, breach_by_year.values, alpha=0.3, color=COLORS['primary'])\n","ax1.set_xlabel('Year', fontsize=12, fontweight='bold')\n","ax1.set_ylabel('Number of Incidents', fontsize=12, fontweight='bold')\n","ax1.set_title('Cybersecurity Incidents Over Time', fontsize=14, fontweight='bold')\n","ax1.grid(True, alpha=0.3)\n","\n","# 2. Incident Type Distribution\n","ax2 = fig.add_subplot(gs[0, 2])\n","action_cols = ['action_malware', 'action_hacking', 'action_social',\n","               'action_misuse', 'action_physical', 'action_error']\n","action_counts = incidents_clean[action_cols].sum().sort_values(ascending=True)\n","action_labels = [col.replace('action_', '').title() for col in action_counts.index]\n","colors_actions = [COLORS['danger'], COLORS['accent'], COLORS['primary'],\n","                  COLORS['secondary'], COLORS['success'], COLORS['neutral']]\n","ax2.barh(action_labels, action_counts.values, color=colors_actions[:len(action_labels)], edgecolor='black')\n","ax2.set_xlabel('Count', fontsize=11, fontweight='bold')\n","ax2.set_title('Incident Types', fontsize=13, fontweight='bold')\n","ax2.grid(axis='x', alpha=0.3)\n","\n","# 3. Records Lost Distribution (Log Scale)\n","ax3 = fig.add_subplot(gs[1, 0])\n","records_with_loss = incidents_clean[incidents_clean['records_lost'] > 0]['records_lost']\n","ax3.hist(np.log10(records_with_loss + 1), bins=30, color=COLORS['danger'],\n","         edgecolor='black', alpha=0.7)\n","ax3.set_xlabel('Log10(Records Lost + 1)', fontsize=11, fontweight='bold')\n","ax3.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n","ax3.set_title('Distribution of Records Lost\\n(Log Scale)', fontsize=13, fontweight='bold')\n","ax3.grid(True, alpha=0.3)\n","\n","# 4. Actor Type Distribution\n","ax4 = fig.add_subplot(gs[1, 1])\n","actor_cols = ['actor_external', 'actor_internal', 'actor_partner']\n","actor_counts = incidents_clean[actor_cols].sum()\n","actor_labels = [col.replace('actor_', '').title() for col in actor_cols]\n","colors_actors = [COLORS['danger'], COLORS['accent'], COLORS['secondary']]\n","wedges, texts, autotexts = ax4.pie(actor_counts.values, labels=actor_labels,\n","                                     autopct='%1.1f%%', startangle=90,\n","                                     colors=colors_actors, textprops={'fontweight': 'bold'})\n","ax4.set_title('Threat Actor Types', fontsize=13, fontweight='bold')\n","\n","# 5. Breach Impact Types\n","ax5 = fig.add_subplot(gs[1, 2])\n","impact_cols = ['confidentiality_breach', 'integrity_breach', 'availability_breach']\n","impact_counts = incidents_clean[impact_cols].sum()\n","impact_labels = [col.replace('_breach', '').title() for col in impact_cols]\n","colors_impact = [COLORS['danger'], COLORS['accent'], COLORS['primary']]\n","bars = ax5.bar(impact_labels, impact_counts.values, color=colors_impact, edgecolor='black')\n","ax5.set_ylabel('Count', fontsize=11, fontweight='bold')\n","ax5.set_title('CIA Triad Impacts', fontsize=13, fontweight='bold')\n","ax5.grid(axis='y', alpha=0.3)\n","for bar in bars:\n","    height = bar.get_height()\n","    ax5.text(bar.get_x() + bar.get_width()/2., height,\n","            f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n","\n","# 6. Financial Data: Asset Distribution\n","ax6 = fig.add_subplot(gs[2, 0])\n","assets_clean = financial_clean['Assets'].dropna()\n","ax6.hist(np.log10(assets_clean + 1), bins=25, color=COLORS['success'],\n","         edgecolor='black', alpha=0.7)\n","ax6.set_xlabel('Log10(Assets + 1)', fontsize=11, fontweight='bold')\n","ax6.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n","ax6.set_title('Company Assets Distribution\\n(Log Scale)', fontsize=13, fontweight='bold')\n","ax6.grid(True, alpha=0.3)\n","\n","# 7. Financial Data: Revenue Distribution\n","ax7 = fig.add_subplot(gs[2, 1])\n","revenue_clean = financial_clean['Revenue'].dropna()\n","ax7.hist(np.log10(revenue_clean + 1), bins=25, color=COLORS['primary'],\n","         edgecolor='black', alpha=0.7)\n","ax7.set_xlabel('Log10(Revenue + 1)', fontsize=11, fontweight='bold')\n","ax7.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n","ax7.set_title('Company Revenue Distribution\\n(Log Scale)', fontsize=13, fontweight='bold')\n","ax7.grid(True, alpha=0.3)\n","\n","# 8. Top Countries by Incidents\n","ax8 = fig.add_subplot(gs[2, 2])\n","top_countries = incidents_clean['victim_country'].value_counts().head(10)\n","ax8.barh(range(len(top_countries)), top_countries.values, color=COLORS['secondary'], edgecolor='black')\n","ax8.set_yticks(range(len(top_countries)))\n","ax8.set_yticklabels(top_countries.index, fontsize=9)\n","ax8.set_xlabel('Number of Incidents', fontsize=11, fontweight='bold')\n","ax8.set_title('Top 10 Countries by Incidents', fontsize=13, fontweight='bold')\n","ax8.grid(axis='x', alpha=0.3)\n","\n","plt.suptitle('Comprehensive Exploratory Data Analysis', fontsize=16, fontweight='bold', y=0.995)\n","plt.show()\n","\n","# Print summary statistics\n","print(\"\\nðŸ“Š Key Summary Statistics:\")\n","print(f\"   Total incidents: {len(incidents_clean):,}\")\n","print(f\"   Date range: {incidents_clean['incident_year'].min()} - {incidents_clean['incident_year'].max()}\")\n","print(f\"   Incidents with data loss: {(incidents_clean['records_lost'] > 0).sum():,}\")\n","print(f\"   Total records lost: {incidents_clean['records_lost'].sum():,.0f}\")\n","print(f\"   Unique countries affected: {incidents_clean['victim_country'].nunique()}\")\n","print(f\"\\n   Financial companies analyzed: {len(financial_clean)}\")\n","print(f\"   Companies with complete data: {financial_clean[feature_columns].dropna().shape[0]}\")\n","print(\"=\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"sruHeCRTxW_8"},"source":["### 3.6 Feature Engineering for Modeling\n","\n","To prepare our data for machine learning models, we need to create a unified dataset that combines information from both sources and engineer relevant features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIljhhllxW_8","executionInfo":{"status":"aborted","timestamp":1763523323826,"user_tz":300,"elapsed":6887,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# FIXED: Now using fuzzy matching results instead of exact matching\n","# Use the matches_df created in cell 12 for proper company matching\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"CREATING BREACH LABELS USING FUZZY MATCHING RESULTS\")\n","print(\"=\"*80)\n","\n","# Get the list of financial companies that were matched to breached companies\n","breached_financial_companies = set(matches_df['financial_name'].unique())\n","\n","print(f\"\\nFinancial companies matched to breach incidents: {len(breached_financial_companies)}\")\n","\n","# Mark companies as breached if they appear in the fuzzy match results\n","financial_clean['breach_occurred'] = financial_clean['company_clean'].isin(breached_financial_companies).astype(int)\n","\n","# Display results\n","print(f\"\\nðŸ“Š Breach Occurrence Statistics:\")\n","print(f\"   Total companies in financial data: {len(financial_clean)}\")\n","print(f\"   Breached companies: {financial_clean['breach_occurred'].sum()}\")\n","print(f\"   Non-breached companies: {(financial_clean['breach_occurred'] == 0).sum()}\")\n","print(f\"   Breach rate: {financial_clean['breach_occurred'].mean()*100:.1f}%\")\n","\n","# Show some example matched companies\n","print(f\"\\nâœ… Sample of matched breached companies:\")\n","breached_examples = financial_clean[financial_clean['breach_occurred'] == 1][['EntityName', 'company_clean']].head(10)\n","for idx, row in breached_examples.iterrows():\n","    print(f\"   - {row['EntityName']}\")\n","\n","print(\"=\"*80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3olszq-ZxW_8","executionInfo":{"status":"aborted","timestamp":1763523323828,"user_tz":300,"elapsed":6885,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Feature Engineering: Financial Health Indicators\n","# Create derived features that may indicate organizational risk\n","\n","# Debt-to-Asset Ratio (higher = more leveraged = potentially higher risk)\n","financial_clean['debt_to_asset_ratio'] = (\n","    financial_clean['Liabilities'] / financial_clean['Assets']\n",").replace([np.inf, -np.inf], np.nan)\n","\n","# Profit Margin (NetIncome / Revenue)\n","financial_clean['profit_margin'] = (\n","    financial_clean['NetIncome'] / financial_clean['Revenue']\n",").replace([np.inf, -np.inf], np.nan)\n","\n","# Log-transformed financial metrics to handle scale\n","for col in ['Assets', 'Revenue', 'Liabilities']:\n","    financial_clean[f'log_{col}'] = np.log1p(financial_clean[col].fillna(0))\n","\n","# Financial stability score (simple composite)\n","# Normalize profit margin and inverse of debt ratio\n","financial_clean['financial_stability'] = (\n","    financial_clean['profit_margin'].fillna(0) -\n","    financial_clean['debt_to_asset_ratio'].fillna(0.5)\n",")\n","\n","print(\"\\n=== Engineered Features Summary ===\")\n","print(financial_clean[[\n","    'debt_to_asset_ratio', 'profit_margin',\n","    'log_Assets', 'financial_stability'\n","]].describe())"]},{"cell_type":"markdown","metadata":{"id":"AvX2WU-mxW_8"},"source":["### 3.7 Data Preparation for Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGcVlnJDxW_8","executionInfo":{"status":"aborted","timestamp":1763523323830,"user_tz":300,"elapsed":6884,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Select features for modeling\n","feature_columns = [\n","    'log_Assets', 'log_Revenue', 'log_Liabilities',\n","    'debt_to_asset_ratio', 'profit_margin', 'financial_stability'\n","]\n","\n","# Create modeling dataset with complete cases\n","modeling_data = financial_clean[feature_columns + ['breach_occurred']].dropna()\n","\n","print(f\"\\nModeling dataset shape: {modeling_data.shape}\")\n","print(f\"Features: {len(feature_columns)}\")\n","print(f\"\\nClass distribution:\")\n","print(modeling_data['breach_occurred'].value_counts())\n","print(f\"\\nClass balance: {modeling_data['breach_occurred'].mean()*100:.2f}% breached\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnv9kDi1xW_8","executionInfo":{"status":"aborted","timestamp":1763523323832,"user_tz":300,"elapsed":6882,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Split data into features and target\n","X = modeling_data[feature_columns]\n","y = modeling_data['breach_occurred']\n","\n","# Train-test split (80-20)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","print(\"\\n=== Train-Test Split ===\")\n","print(f\"Training set: {X_train.shape}\")\n","print(f\"Test set: {X_test.shape}\")\n","print(f\"\\nTraining set breach rate: {y_train.mean()*100:.2f}%\")\n","print(f\"Test set breach rate: {y_test.mean()*100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"niBwJRvPxW_9"},"source":["### 3.8 Model 1: Clustering Analysis (K-Means)\n","\n","We use K-Means clustering to identify natural groupings of companies based on their financial characteristics, then examine whether these clusters correspond to different breach rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9s46V1MxW_9","executionInfo":{"status":"aborted","timestamp":1763523323835,"user_tz":300,"elapsed":6881,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Determine optimal number of clusters using elbow method\n","inertias = []\n","silhouette_scores = []\n","K_range = range(2, 11)\n","\n","for k in K_range:\n","    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n","    kmeans.fit(X_train_scaled)\n","    inertias.append(kmeans.inertia_)\n","    silhouette_scores.append(silhouette_score(X_train_scaled, kmeans.labels_))\n","\n","# Visualize elbow curve and silhouette scores\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Elbow curve\n","ax1.plot(K_range, inertias, marker='o', linewidth=2, markersize=8, color=COLORS['primary'])\n","ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n","ax1.set_ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)\n","ax1.set_title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n","ax1.grid(True, alpha=0.3)\n","\n","# Silhouette scores\n","ax2.plot(K_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color=COLORS['secondary'])\n","ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n","ax2.set_ylabel('Silhouette Score', fontsize=12)\n","ax2.set_title('Silhouette Score by Number of Clusters', fontsize=14, fontweight='bold')\n","ax2.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nSilhouette Scores by k:\")\n","for k, score in zip(K_range, silhouette_scores):\n","    print(f\"k={k}: {score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-NL0Zc6xW_9","executionInfo":{"status":"aborted","timestamp":1763523323837,"user_tz":300,"elapsed":6879,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Apply K-Means with optimal k (using k=4 based on typical elbow/silhouette analysis)\n","optimal_k = 4\n","kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n","cluster_labels_train = kmeans.fit_predict(X_train_scaled)\n","cluster_labels_test = kmeans.predict(X_test_scaled)\n","\n","# Analyze breach rates by cluster\n","train_cluster_df = pd.DataFrame({\n","    'cluster': cluster_labels_train,\n","    'breached': y_train.values\n","})\n","\n","cluster_breach_rates = train_cluster_df.groupby('cluster')['breached'].agg(['mean', 'count'])\n","cluster_breach_rates.columns = ['breach_rate', 'company_count']\n","cluster_breach_rates = cluster_breach_rates.sort_values('breach_rate', ascending=False)\n","\n","print(f\"\\n=== K-Means Clustering Results (k={optimal_k}) ===\")\n","print(f\"Silhouette Score: {silhouette_score(X_train_scaled, cluster_labels_train):.4f}\")\n","print(\"\\nBreach Rates by Cluster:\")\n","print(cluster_breach_rates)\n","\n","# Visualize clusters and breach rates\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","# FIXED: Use consistent color mapping for both charts\n","# Create color map that maps cluster ID to color\n","cluster_color_map = {i: colors_cluster[i] for i in range(optimal_k)}\n","colors_cluster = [COLORS['primary'], COLORS['secondary'], COLORS['accent'], COLORS['success']]\n","\n","# Cluster sizes (by cluster ID order)\n","cluster_sizes = train_cluster_df['cluster'].value_counts().sort_index()\n","colors_for_sizes = [cluster_color_map[i] for i in cluster_sizes.index]\n","ax1.bar(cluster_sizes.index, cluster_sizes.values, color=colors_for_sizes, edgecolor='black')\n","ax1.set_xlabel('Cluster ID', fontsize=12)\n","ax1.set_ylabel('Number of Companies', fontsize=12)\n","ax1.set_title('Company Distribution Across Clusters', fontsize=14, fontweight='bold')\n","ax1.grid(axis='y', alpha=0.3)\n","\n","# FIXED: Breach rates by cluster (maintain cluster ID order for color alignment)\n","# Get breach rates sorted by cluster ID (not by breach rate) for proper color mapping\n","breach_rates_by_id = train_cluster_df.groupby('cluster')['breached'].mean().sort_index()\n","colors_for_breach = [cluster_color_map[i] for i in breach_rates_by_id.index]\n","bars = ax2.bar(breach_rates_by_id.index, breach_rates_by_id.values * 100,\n","               color=colors_for_breach, edgecolor='black')\n","ax2.set_xlabel('Cluster ID', fontsize=12)\n","ax2.set_ylabel('Breach Rate (%)', fontsize=12)\n","ax2.set_title('Breach Rate by Cluster', fontsize=14, fontweight='bold')\n","ax2.grid(axis='y', alpha=0.3)\n","\n","# Add value labels on bars\n","for bar in bars:\n","    height = bar.get_height()\n","    ax2.text(bar.get_x() + bar.get_width()/2., height,\n","            f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bm-6-A24xW_9"},"source":["### 3.9 Model 2: Linear Regression for Breach Severity\n","\n","We use linear regression to examine the relationship between financial indicators and breach severity (measured by records lost)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HFKGH4pxW_9","executionInfo":{"status":"aborted","timestamp":1763523323839,"user_tz":300,"elapsed":6877,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# FIXED: Use fuzzy matching results to merge financial and incident data\n","# Prepare data for regression (only companies with breaches and records_lost data)\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"LINEAR REGRESSION DATA PREPARATION\")\n","print(\"=\"*80)\n","\n","# Get incidents with records lost data\n","incidents_with_loss = incidents_clean[\n","    (incidents_clean['records_lost'] > 0) &\n","    (incidents_clean['victim_name'].notna())\n","].copy()\n","\n","incidents_with_loss['company_clean'] = incidents_with_loss['victim_name'].str.strip().str.lower()\n","\n","print(f\"\\nIncidents with records lost: {len(incidents_with_loss)}\")\n","\n","# FIXED: Use the fuzzy matching dataframe to map incident companies to financial companies\n","# Create a mapping from incident_name to financial_name\n","incident_to_financial_map = dict(zip(matches_df['incident_name'], matches_df['financial_name']))\n","\n","# Map incident company names to financial company names\n","incidents_with_loss['financial_company_clean'] = incidents_with_loss['company_clean'].map(incident_to_financial_map)\n","\n","# Merge with financial data using the mapped names\n","regression_data = incidents_with_loss[incidents_with_loss['financial_company_clean'].notna()].merge(\n","    financial_clean[['company_clean'] + feature_columns],\n","    left_on='financial_company_clean',\n","    right_on='company_clean',\n","    how='inner'\n",")\n","\n","# Use log of records lost to handle skewness\n","regression_data['log_records_lost'] = np.log1p(regression_data['records_lost'])\n","\n","print(f\"\\nðŸ“Š Regression Dataset Statistics:\")\n","print(f\"   Total incidents with records lost: {len(incidents_with_loss):,}\")\n","print(f\"   Incidents matched to financial data: {len(regression_data)}\")\n","print(f\"   Match rate: {len(regression_data)/len(incidents_with_loss)*100:.1f}%\")\n","\n","if len(regression_data) > 0:\n","    print(f\"\\n   Records lost statistics:\")\n","    print(f\"   Min: {regression_data['records_lost'].min():,.0f}\")\n","    print(f\"   Max: {regression_data['records_lost'].max():,.0f}\")\n","    print(f\"   Mean: {regression_data['records_lost'].mean():,.0f}\")\n","    print(f\"   Median: {regression_data['records_lost'].median():,.0f}\")\n","\n","print(\"=\"*80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeE-mFZoxW_9","executionInfo":{"status":"aborted","timestamp":1763523323840,"user_tz":300,"elapsed":6874,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# FIXED: Improved linear regression with better data handling\n","# Prepare regression features and target\n","if len(regression_data) > 10:  # Need sufficient data points\n","    # Drop rows with missing values in feature columns\n","    X_reg = regression_data[feature_columns].dropna()\n","    y_reg = regression_data.loc[X_reg.index, 'log_records_lost']\n","\n","    # Train-test split\n","    X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n","        X_reg, y_reg, test_size=0.2, random_state=42\n","    )\n","\n","    # Fit linear regression\n","    lr_model = LinearRegression()\n","    lr_model.fit(X_reg_train, y_reg_train)\n","\n","    # Predictions\n","    y_reg_pred_train = lr_model.predict(X_reg_train)\n","    y_reg_pred_test = lr_model.predict(X_reg_test)\n","\n","    # Evaluation metrics\n","    train_r2 = r2_score(y_reg_train, y_reg_pred_train)\n","    test_r2 = r2_score(y_reg_test, y_reg_pred_test)\n","    train_rmse = np.sqrt(mean_squared_error(y_reg_train, y_reg_pred_train))\n","    test_rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred_test))\n","\n","    print(\"\\n=== Linear Regression Results ===\")\n","    print(f\"Training samples: {len(X_reg_train)}\")\n","    print(f\"Test samples: {len(X_reg_test)}\")\n","    print(f\"Training RÂ²: {train_r2:.4f}\")\n","    print(f\"Test RÂ²: {test_r2:.4f}\")\n","    print(f\"Training RMSE: {train_rmse:.4f}\")\n","    print(f\"Test RMSE: {test_rmse:.4f}\")\n","\n","    # Feature coefficients\n","    coef_df = pd.DataFrame({\n","        'Feature': feature_columns,\n","        'Coefficient': lr_model.coef_\n","    }).sort_values('Coefficient', key=abs, ascending=False)\n","\n","    print(\"\\nFeature Coefficients (sorted by absolute value):\")\n","    print(coef_df)\n","\n","    # Visualization: Actual vs Predicted\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","    # Scatter plot: Actual vs Predicted\n","    ax1.scatter(y_reg_test, y_reg_pred_test, alpha=0.6, color=COLORS['primary'], s=100, edgecolor='black')\n","    ax1.plot([y_reg_test.min(), y_reg_test.max()],\n","             [y_reg_test.min(), y_reg_test.max()],\n","             'r--', lw=3, label='Perfect Prediction')\n","    ax1.set_xlabel('Actual Log(Records Lost)', fontsize=12, fontweight='bold')\n","    ax1.set_ylabel('Predicted Log(Records Lost)', fontsize=12, fontweight='bold')\n","    ax1.set_title(f'Actual vs Predicted (Test RÂ² = {test_r2:.4f})',\n","                  fontsize=14, fontweight='bold')\n","    ax1.legend(fontsize=11)\n","    ax1.grid(True, alpha=0.3)\n","\n","    # Feature importance (absolute coefficients)\n","    colors_coef = [COLORS['success'] if c > 0 else COLORS['danger'] for c in coef_df['Coefficient']]\n","    ax2.barh(coef_df['Feature'], coef_df['Coefficient'].abs(), color=colors_coef, edgecolor='black')\n","    ax2.set_xlabel('Absolute Coefficient Value', fontsize=12, fontweight='bold')\n","    ax2.set_ylabel('Feature', fontsize=12, fontweight='bold')\n","    ax2.set_title('Feature Importance in Predicting Breach Severity',\n","                  fontsize=14, fontweight='bold')\n","    ax2.grid(axis='x', alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","elif len(regression_data) > 0:\n","    print(f\"\\nâš ï¸ Insufficient data for reliable regression analysis\")\n","    print(f\"   Found {len(regression_data)} matched incidents with records lost\")\n","    print(f\"   Minimum recommended: 10+ samples\")\n","    print(f\"   However, we CAN still show the available data:\")\n","\n","    # Show what data we have\n","    X_reg = regression_data[feature_columns].dropna()\n","    y_reg = regression_data.loc[X_reg.index, 'log_records_lost']\n","\n","    if len(X_reg) >= 3:\n","        # Fit model anyway with warning\n","        lr_model = LinearRegression()\n","        lr_model.fit(X_reg, y_reg)\n","\n","        print(f\"\\nâš ï¸ Fitting model with only {len(X_reg)} samples (results may be unreliable)\")\n","\n","        # Feature coefficients\n","        coef_df = pd.DataFrame({\n","            'Feature': feature_columns,\n","            'Coefficient': lr_model.coef_\n","        }).sort_values('Coefficient', key=abs, ascending=False)\n","\n","        print(\"\\nFeature Coefficients:\")\n","        print(coef_df)\n","\n","        # Simple visualization\n","        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n","        colors_coef = [COLORS['success'] if c > 0 else COLORS['danger'] for c in coef_df['Coefficient']]\n","        ax.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors_coef, edgecolor='black')\n","        ax.set_xlabel('Coefficient Value', fontsize=12, fontweight='bold')\n","        ax.set_ylabel('Feature', fontsize=12, fontweight='bold')\n","        ax.set_title(f'Feature Coefficients (n={len(X_reg)} samples - LOW CONFIDENCE)',\n","                      fontsize=14, fontweight='bold')\n","        ax.grid(axis='x', alpha=0.3)\n","        ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(f\"   Only {len(X_reg)} complete samples - cannot fit model\")\n","else:\n","    print(\"\\nâš ï¸ No matching data for regression analysis\")\n","    print(\"   This occurs when:\")\n","    print(\"   1. No companies in financial dataset experienced breaches with records lost\")\n","    print(\"   2. Fuzzy matching didn't link any breach incidents to financial companies\")\n","    print(\"   Recommendation: Review fuzzy matching thresholds or collect more data\")"]},{"cell_type":"markdown","metadata":{"id":"Yw9WbwuDxW_-"},"source":["### 3.10 Model 3: Decision Tree Classification\n","\n","Decision trees provide interpretable rules for predicting breach likelihood and identify the most influential features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPANFYcgxW_-","executionInfo":{"status":"aborted","timestamp":1763523323842,"user_tz":300,"elapsed":6873,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Train decision tree classifier\n","dt_model = DecisionTreeClassifier(\n","    max_depth=5,  # Limit depth for interpretability\n","    min_samples_split=20,\n","    min_samples_leaf=10,\n","    random_state=42\n",")\n","\n","dt_model.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred_dt_train = dt_model.predict(X_train)\n","y_pred_dt_test = dt_model.predict(X_test)\n","y_pred_dt_proba = dt_model.predict_proba(X_test)[:, 1]\n","\n","# Evaluation\n","dt_train_acc = accuracy_score(y_train, y_pred_dt_train)\n","dt_test_acc = accuracy_score(y_test, y_pred_dt_test)\n","dt_precision = precision_score(y_test, y_pred_dt_test, zero_division=0)\n","dt_recall = recall_score(y_test, y_pred_dt_test, zero_division=0)\n","dt_f1 = f1_score(y_test, y_pred_dt_test, zero_division=0)\n","\n","print(\"\\n=== Decision Tree Results ===\")\n","print(f\"Training Accuracy: {dt_train_acc:.4f}\")\n","print(f\"Test Accuracy: {dt_test_acc:.4f}\")\n","print(f\"Precision: {dt_precision:.4f}\")\n","print(f\"Recall: {dt_recall:.4f}\")\n","print(f\"F1 Score: {dt_f1:.4f}\")\n","\n","# Feature importance\n","feature_importance_dt = pd.DataFrame({\n","    'Feature': feature_columns,\n","    'Importance': dt_model.feature_importances_\n","}).sort_values('Importance', ascending=False)\n","\n","print(\"\\nFeature Importance:\")\n","print(feature_importance_dt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsY9CUOoxW_-","executionInfo":{"status":"aborted","timestamp":1763523323843,"user_tz":300,"elapsed":6870,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Visualize decision tree and feature importance\n","fig = plt.figure(figsize=(16, 10))\n","\n","# Decision tree visualization\n","ax1 = plt.subplot(2, 1, 1)\n","plot_tree(dt_model,\n","          feature_names=feature_columns,\n","          class_names=['No Breach', 'Breach'],\n","          filled=True,\n","          rounded=True,\n","          fontsize=10,\n","          ax=ax1)\n","ax1.set_title('Decision Tree Structure (max_depth=5)', fontsize=14, fontweight='bold')\n","\n","# Feature importance bar chart\n","ax2 = plt.subplot(2, 1, 2)\n","colors_importance = plt.cm.viridis(feature_importance_dt['Importance'] / feature_importance_dt['Importance'].max())\n","bars = ax2.barh(feature_importance_dt['Feature'],\n","                feature_importance_dt['Importance'],\n","                color=colors_importance,\n","                edgecolor='black')\n","ax2.set_xlabel('Importance Score', fontsize=12)\n","ax2.set_ylabel('Feature', fontsize=12)\n","ax2.set_title('Feature Importance in Decision Tree', fontsize=14, fontweight='bold')\n","ax2.grid(axis='x', alpha=0.3)\n","\n","# Add value labels\n","for bar in bars:\n","    width = bar.get_width()\n","    ax2.text(width, bar.get_y() + bar.get_height()/2.,\n","            f'{width:.3f}', ha='left', va='center', fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6Pslxa-IxW_-"},"source":["### 3.11 Model 4: Logistic Regression Classification\n","\n","Logistic regression provides a probabilistic baseline model for breach prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XofXiRuQxW_-","executionInfo":{"status":"aborted","timestamp":1763523323845,"user_tz":300,"elapsed":6867,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# Train logistic regression\n","logreg_model = LogisticRegression(\n","    max_iter=1000,\n","    random_state=42,\n","    class_weight='balanced'  # Handle class imbalance\n",")\n","\n","logreg_model.fit(X_train_scaled, y_train)\n","\n","# Predictions\n","y_pred_lr_train = logreg_model.predict(X_train_scaled)\n","y_pred_lr_test = logreg_model.predict(X_test_scaled)\n","y_pred_lr_proba = logreg_model.predict_proba(X_test_scaled)[:, 1]\n","\n","# Evaluation\n","lr_train_acc = accuracy_score(y_train, y_pred_lr_train)\n","lr_test_acc = accuracy_score(y_test, y_pred_lr_test)\n","lr_precision = precision_score(y_test, y_pred_lr_test, zero_division=0)\n","lr_recall = recall_score(y_test, y_pred_lr_test, zero_division=0)\n","lr_f1 = f1_score(y_test, y_pred_lr_test, zero_division=0)\n","\n","# ROC-AUC (if we have positive cases)\n","if len(np.unique(y_test)) > 1:\n","    lr_auc = roc_auc_score(y_test, y_pred_lr_proba)\n","else:\n","    lr_auc = None\n","\n","print(\"\\n=== Logistic Regression Results ===\")\n","print(f\"Training Accuracy: {lr_train_acc:.4f}\")\n","print(f\"Test Accuracy: {lr_test_acc:.4f}\")\n","print(f\"Precision: {lr_precision:.4f}\")\n","print(f\"Recall: {lr_recall:.4f}\")\n","print(f\"F1 Score: {lr_f1:.4f}\")\n","if lr_auc:\n","    print(f\"ROC-AUC: {lr_auc:.4f}\")\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred_lr_test,\n","                          target_names=['No Breach', 'Breach'],\n","                          zero_division=0))"]},{"cell_type":"markdown","metadata":{"id":"U_FCIZalxW_-"},"source":["### 3.12 Model 5: XGBoost ClassificationXGBoost combines multiple decision trees to improve prediction accuracy and robustness."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TX2C-UfMxW_-","executionInfo":{"status":"aborted","timestamp":1763523323846,"user_tz":300,"elapsed":6864,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# FIXED: Corrected variable names and imports\n","# Calculate scale_pos_weight for class imbalance\n","n_neg = (y_train == 0).sum()\n","n_pos = (y_train == 1).sum()\n","scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0\n","\n","# Train XGBoost\n","xgb_model = XGBClassifier(\n","    n_estimators=200,\n","    max_depth=6,\n","    learning_rate=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    scale_pos_weight=scale_pos_weight,\n","    random_state=RANDOM_STATE,\n","    eval_metric='logloss',\n","    use_label_encoder=False\n",")\n","\n","xgb_model.fit(X_train, y_train, verbose=False)\n","\n","# Predictions (FIXED: Changed from y_pred_rf_test to y_pred_xgb_test)\n","y_pred_xgb_train = xgb_model.predict(X_train)\n","y_pred_xgb_test = xgb_model.predict(X_test)\n","y_pred_xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n","\n","# Evaluation\n","xgb_train_acc = accuracy_score(y_train, y_pred_xgb_train)\n","xgb_test_acc = accuracy_score(y_test, y_pred_xgb_test)\n","xgb_precision = precision_score(y_test, y_pred_xgb_test, zero_division=0)\n","xgb_recall = recall_score(y_test, y_pred_xgb_test, zero_division=0)\n","xgb_f1 = f1_score(y_test, y_pred_xgb_test, zero_division=0)\n","\n","if len(np.unique(y_test)) > 1:\n","    xgb_auc = roc_auc_score(y_test, y_pred_xgb_proba)\n","else:\n","    xgb_auc = 0.0\n","\n","print(\"\\n=== XGBoost Results ===\")\n","print(f\"Training Accuracy: {xgb_train_acc:.4f}\")\n","print(f\"Test Accuracy: {xgb_test_acc:.4f}\")\n","print(f\"Precision: {xgb_precision:.4f}\")\n","print(f\"Recall: {xgb_recall:.4f}\")\n","print(f\"F1 Score: {xgb_f1:.4f}\")\n","if xgb_auc > 0:\n","    print(f\"ROC-AUC: {xgb_auc:.4f}\")\n","\n","# Feature importance\n","feature_importance_xgb = pd.DataFrame({\n","    'Feature': feature_columns,\n","    'Importance': xgb_model.feature_importances_\n","}).sort_values('Importance', ascending=False)\n","\n","print(\"\\n=== Top Features ===\")\n","print(feature_importance_xgb)\n","\n","print(\"\\n=== Classification Report ===\")\n","print(classification_report(y_test, y_pred_xgb_test,\n","                          target_names=['No Breach', 'Breach'], zero_division=0))"]},{"cell_type":"markdown","metadata":{"id":"Hef7DC1CxW__"},"source":["### 3.13 Comprehensive Model Comparison Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeeIPSyuxW__","executionInfo":{"status":"aborted","timestamp":1763523323848,"user_tz":300,"elapsed":6862,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# FIXED: Corrected all variable references from RF to XGBoost\n","# Create comprehensive comparison visualizations\n","fig = plt.figure(figsize=(18, 12))\n","gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n","\n","# 1. Model Accuracy Comparison\n","ax1 = fig.add_subplot(gs[0, 0])\n","models = ['Logistic\\nRegression', 'Decision\\nTree', 'XGBoost']\n","train_accs = [lr_train_acc, dt_train_acc, xgb_train_acc]\n","test_accs = [lr_test_acc, dt_test_acc, xgb_test_acc]\n","\n","x = np.arange(len(models))\n","width = 0.35\n","\n","bars1 = ax1.bar(x - width/2, train_accs, width, label='Training',\n","                color=COLORS['primary'], edgecolor='black')\n","bars2 = ax1.bar(x + width/2, test_accs, width, label='Test',\n","                color=COLORS['accent'], edgecolor='black')\n","\n","ax1.set_ylabel('Accuracy', fontsize=12)\n","ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n","ax1.set_xticks(x)\n","ax1.set_xticklabels(models)\n","ax1.legend()\n","ax1.grid(axis='y', alpha=0.3)\n","ax1.set_ylim([0, 1.1])\n","\n","# Add value labels\n","for bars in [bars1, bars2]:\n","    for bar in bars:\n","        height = bar.get_height()\n","        ax1.text(bar.get_x() + bar.get_width()/2., height,\n","                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n","\n","# 2. Precision-Recall-F1 Comparison\n","ax2 = fig.add_subplot(gs[0, 1])\n","metrics_data = {\n","    'Precision': [lr_precision, dt_precision, xgb_precision],\n","    'Recall': [lr_recall, dt_recall, xgb_recall],\n","    'F1 Score': [lr_f1, dt_f1, xgb_f1]\n","}\n","\n","x = np.arange(len(models))\n","width = 0.25\n","multiplier = 0\n","\n","colors_metrics = [COLORS['success'], COLORS['secondary'], COLORS['danger']]\n","\n","for i, (metric, values) in enumerate(metrics_data.items()):\n","    offset = width * multiplier\n","    ax2.bar(x + offset, values, width, label=metric,\n","            color=colors_metrics[i], edgecolor='black')\n","    multiplier += 1\n","\n","ax2.set_ylabel('Score', fontsize=12)\n","ax2.set_title('Classification Metrics Comparison', fontsize=14, fontweight='bold')\n","ax2.set_xticks(x + width)\n","ax2.set_xticklabels(models)\n","ax2.legend(loc='upper right')\n","ax2.grid(axis='y', alpha=0.3)\n","ax2.set_ylim([0, 1.1])\n","\n","# 3. ROC Curves (if applicable)\n","ax3 = fig.add_subplot(gs[1, :])\n","if lr_auc and xgb_auc:\n","    # Compute ROC curves\n","    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_lr_proba)\n","    fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_dt_proba)\n","    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_xgb_proba)\n","\n","    # Compute DT AUC\n","    dt_auc = roc_auc_score(y_test, y_pred_dt_proba)\n","\n","    ax3.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {lr_auc:.3f})',\n","            linewidth=2, color=COLORS['primary'])\n","    ax3.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {dt_auc:.3f})',\n","            linewidth=2, color=COLORS['secondary'])\n","    ax3.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {xgb_auc:.3f})',\n","            linewidth=2, color=COLORS['accent'])\n","    ax3.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n","\n","    ax3.set_xlabel('False Positive Rate', fontsize=12)\n","    ax3.set_ylabel('True Positive Rate', fontsize=12)\n","    ax3.set_title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n","    ax3.legend(loc='lower right')\n","    ax3.grid(True, alpha=0.3)\n","else:\n","    ax3.text(0.5, 0.5, 'ROC curves unavailable\\n(insufficient positive cases)',\n","            ha='center', va='center', fontsize=14)\n","    ax3.set_xlim([0, 1])\n","    ax3.set_ylim([0, 1])\n","\n","# 4. Confusion Matrix - XGBoost (FIXED: Changed from RF to XGBoost)\n","ax4 = fig.add_subplot(gs[2, 0])\n","cm_xgb = confusion_matrix(y_test, y_pred_xgb_test)\n","sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=['No Breach', 'Breach'],\n","            yticklabels=['No Breach', 'Breach'],\n","            ax=ax4, cbar_kws={'label': 'Count'})\n","ax4.set_ylabel('Actual', fontsize=12)\n","ax4.set_xlabel('Predicted', fontsize=12)\n","ax4.set_title('Confusion Matrix - XGBoost', fontsize=14, fontweight='bold')\n","\n","# 5. Feature Importance Comparison\n","ax5 = fig.add_subplot(gs[2, 1])\n","importance_comparison = pd.DataFrame({\n","    'Feature': feature_columns,\n","    'Decision Tree': dt_model.feature_importances_,\n","    'XGBoost': xgb_model.feature_importances_\n","})\n","\n","# Normalize for comparison\n","importance_comparison['DT_norm'] = (\n","    importance_comparison['Decision Tree'] / importance_comparison['Decision Tree'].sum())\n","importance_comparison['XGB_norm'] = (\n","    importance_comparison['XGBoost'] / importance_comparison['XGBoost'].sum())\n","\n","x = np.arange(len(feature_columns))\n","width = 0.35\n","\n","ax5.barh(x - width/2, importance_comparison['DT_norm'], width,\n","         label='Decision Tree', color=COLORS['secondary'], edgecolor='black')\n","ax5.barh(x + width/2, importance_comparison['XGB_norm'], width,\n","        label='XGBoost', color=COLORS['accent'], edgecolor='black')\n","\n","ax5.set_yticks(x)\n","ax5.set_yticklabels(feature_columns)\n","ax5.set_xlabel('Normalized Importance', fontsize=12)\n","ax5.set_title('Feature Importance Comparison', fontsize=14, fontweight='bold')\n","ax5.legend()\n","ax5.grid(axis='x', alpha=0.3)\n","\n","plt.suptitle('Comprehensive Model Performance Analysis',\n","             fontsize=16, fontweight='bold', y=0.995)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lSPNijpjxW__"},"source":["### 3.14 Model Performance Summary Table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gP1Kq-NtxW__","executionInfo":{"status":"aborted","timestamp":1763523323849,"user_tz":300,"elapsed":6859,"user":{"displayName":"Thomas Powell","userId":"17214006362460852337"}}},"outputs":[],"source":["# FIXED: Corrected model references and AUC calculation\n","# Create comprehensive performance summary\n","performance_summary = pd.DataFrame({\n","    'Model': ['Logistic Regression', 'Decision Tree', 'XGBoost'],\n","    'Train Accuracy': [lr_train_acc, dt_train_acc, xgb_train_acc],\n","    'Test Accuracy': [lr_test_acc, dt_test_acc, xgb_test_acc],\n","    'Precision': [lr_precision, dt_precision, xgb_precision],\n","    'Recall': [lr_recall, dt_recall, xgb_recall],\n","    'F1 Score': [lr_f1, dt_f1, xgb_f1],\n","    'ROC-AUC': [\n","        lr_auc if lr_auc else np.nan,\n","        roc_auc_score(y_test, y_pred_dt_proba) if len(np.unique(y_test)) > 1 else np.nan,\n","        xgb_auc if xgb_auc else np.nan\n","    ]\n","})\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n","print(\"=\"*80)\n","print(performance_summary.to_string(index=False))\n","print(\"\\n\" + \"=\"*80)\n","\n","# Determine best model based on F1 score (balanced metric)\n","best_model_idx = performance_summary['F1 Score'].idxmax()\n","best_model_name = performance_summary.loc[best_model_idx, 'Model']\n","\n","print(f\"\\nðŸ† Best Performing Model: {best_model_name}\")\n","print(f\"   Based on F1 Score: {performance_summary.loc[best_model_idx, 'F1 Score']:.4f}\")\n","print(f\"   Test Accuracy: {performance_summary.loc[best_model_idx, 'Test Accuracy']:.4f}\")\n","print(f\"   Precision: {performance_summary.loc[best_model_idx, 'Precision']:.4f}\")\n","print(f\"   Recall: {performance_summary.loc[best_model_idx, 'Recall']:.4f}\")\n","\n","print(\"\\n\" + \"=\"*80)"]},{"cell_type":"markdown","metadata":{"id":"4jjWbhJXxW__"},"source":["---\n","## 4. Evaluation\n","\n","<!-- THOMAS: Write evaluation section here -->\n","<!-- Include: interpretation of metrics, comparison of models, -->\n","<!-- answers to research questions, discussion of results -->\n","\n","*THOMAS: This section should evaluate all models using the appropriate metrics. Discuss:*\n","- *How well each model performed*\n","- *Which models worked best for different aspects of the problem*\n","- *Whether we were able to answer our initial questions*\n","- *Interpretation of clustering results, regression coefficients, and classification performance*"]},{"cell_type":"markdown","metadata":{"id":"HlKiIkXTxW__"},"source":["---\n","## 5. Storytelling and Conclusion\n","\n","<!-- RAFAN: Write storytelling section here -->\n","<!-- Include: key insights, narrative of what we learned, -->\n","<!-- patterns discovered, practical implications -->\n","\n","*RAFAN: This section should tell the story of what we discovered. Include:*\n","- *What insights were gained through the project?*\n","- *Were we able to answer our initial questions?*\n","- *What patterns or relationships did we find?*\n","- *What stories can the data tell us about cybersecurity breaches?*"]},{"cell_type":"markdown","metadata":{"id":"tPFT67mVxW__"},"source":["### 5.1 Conclusion\n","\n","<!-- ALL TEAM MEMBERS: Contribute to conclusion -->\n","<!-- Each person should add their perspective on: -->\n","<!-- - Key learnings from the project -->\n","<!-- - What worked well / what didn't -->\n","<!-- - Future improvements -->\n","<!-- - Lessons learned from the class -->\n","\n","*ALL: Each team member should contribute to the conclusion, discussing:*\n","- *What you learned throughout this project*\n","- *What you learned throughout the entire class*\n","- *Future steps or improvements that could be made*\n","- *Critical thinking about the project's approach and results*"]},{"cell_type":"markdown","metadata":{"id":"fwOhxF9GxXAA"},"source":["---\n","## 6. Impact Section\n","\n","<!-- ALL TEAM MEMBERS: Contribute to impact discussion -->\n","\n","*ALL: Discuss the impact of this project (social, ethical, etc.):*\n","- *How could this work be used positively?*\n","- *What are the potential negative impacts or misuses?*\n","- *Ethical considerations in predicting cybersecurity breaches*\n","- *Privacy and fairness concerns*\n","- *Who could be affected by this type of analysis?*"]},{"cell_type":"markdown","metadata":{"id":"lhtdR1vkxXAB"},"source":["---\n","## 7. References and Data Access\n","\n","### Data Sources\n","\n","1. **VERIS Community Database (VCDB)**\n","   - Source: https://github.com/vz-risk/VCDB\n","   - Description: Structured cybersecurity incident data\n","   - File: `vcdb_cybersecurity_incidents.csv`\n","\n","2. **SEC EDGAR Company Facts**\n","   - Source: https://www.sec.gov/data-research/sec-markets-data/financial-statement-data-sets\n","   - Description: Financial statement data for public companies\n","   - File: `sec_company_financials.csv`\n","\n","### GitHub Repository\n","\n","*Include link to your GitHub repository here*\n","\n","### Tools and Libraries\n","\n","- Python 3.x\n","- pandas, numpy - Data manipulation\n","- scikit-learn - Machine learning models\n","- matplotlib, seaborn - Visualization\n","\n","---\n","\n","print(f\"   F1 Score: {performance_summary.loc[best_model_idx, 'F1 Score']:.4f}\")\n","print(\"=\"*100)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ITCS3162","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":0}